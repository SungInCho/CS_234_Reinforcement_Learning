{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "24f62cfa-9abc-4ffd-af97-5a2319353b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from riverswim import RiverSwim\n",
    "import copy \n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a32ba762-4c91-4ece-a480-6bc2fc5a2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_backup(state, action, R, T, gamma, V):\n",
    "    \"\"\"\n",
    "    Perform a single Bellman backup.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state: int\n",
    "    action: int\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "    gamma: float\n",
    "    V: np.array (num_states)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    backup_val: float\n",
    "    \"\"\"\n",
    "    backup_val = 0.\n",
    "    \n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    backup_val = R[state, action] + gamma * np.sum(T[state, action] * V)\n",
    "    ############################\n",
    "\n",
    "    return backup_val\n",
    "\n",
    "def policy_evaluation(policy, R, T, gamma, tol=1e-3):\n",
    "    \"\"\"\n",
    "    Compute the value function induced by a given policy for the input MDP\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.array (num_states)\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "    gamma: float\n",
    "    tol: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    value_function = np.zeros(num_states)\n",
    "    \n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    while True:\n",
    "        V = np.copy(value_function)\n",
    "        for state in range(num_states):\n",
    "            value_function[state] = bellman_backup(state, policy[state], R, T, gamma, V)\n",
    "        if np.linalg.norm(V - value_function, ord=np.inf) < tol:\n",
    "            break\n",
    "    ############################\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def policy_improvement(policy, R, T, V_policy, gamma):\n",
    "    \"\"\"\n",
    "    Given the value function induced by a given policy, perform policy improvement\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: np.array (num_states)\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "    V_policy: np.array (num_states)\n",
    "    gamma: float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_policy: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    new_policy = np.zeros(num_states, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    new_policy = np.argmax(R + gamma * np.sum(T * V_policy, axis=2), axis=1)\n",
    "    ############################\n",
    "    \n",
    "    return new_policy\n",
    "\n",
    "\n",
    "def policy_iteration(R, T, gamma, tol=1e-3):\n",
    "    \"\"\"Runs policy iteration.\n",
    "\n",
    "    You should call the policy_evaluation() and policy_improvement() methods to\n",
    "    implement this method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    R: np.array (num_states, num_actions)\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V_policy: np.array (num_states)\n",
    "    policy: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    V_policy = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    iter = 0\n",
    "    while True:    \n",
    "        iter += 1\n",
    "        V_policy = policy_evaluation(policy, R, T, gamma, tol)\n",
    "        new_policy = policy_improvement(policy, R, T, V_policy, gamma)\n",
    "\n",
    "        if np.linalg.norm(policy - new_policy, ord=1) < tol:\n",
    "            break\n",
    "        else:\n",
    "            policy = new_policy\n",
    "    # print(f'Policy Iteration Rounds: {iter}')\n",
    "    ############################\n",
    "    \n",
    "    return V_policy, policy\n",
    "\n",
    "\n",
    "def value_iteration(R, T, gamma, tol=1e-3):\n",
    "    \"\"\"Runs value iteration.\n",
    "    Parameters\n",
    "    ----------\n",
    "    R: np.array (num_states, num_actions)  b\n",
    "    T: np.array (num_states, num_actions, num_states)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value_function: np.array (num_states)\n",
    "    policy: np.array (num_states)\n",
    "    \"\"\"\n",
    "    num_states, num_actions = R.shape\n",
    "    value_function = np.zeros(num_states)\n",
    "    policy = np.zeros(num_states, dtype=int)\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    iter = 0\n",
    "    while True:\n",
    "        iter += 1\n",
    "        V = np.copy(value_function)\n",
    "        value_function = np.max(R + gamma * np.sum(T * V, axis=2), axis=1)\n",
    "        if np.linalg.norm(value_function - V, ord=np.inf) < tol:\n",
    "            break\n",
    "    # print(f'Value Iternation Rounds: {iter}')\n",
    "\n",
    "    policy = np.argmax(R + gamma * np.sum(T * value_function, axis=2), axis=1)\n",
    "    ############################\n",
    "    \n",
    "    return value_function, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "4c30e9d1-6060-4210-ac7f-69100a78e2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "[0.01  0.005 0.017 0.076 0.342 1.526]\n",
      "['L', 'L', 'R', 'R', 'R', 'R']\n",
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "[0.01  0.005 0.017 0.076 0.342 1.526]\n",
      "['L', 'L', 'R', 'R', 'R', 'R']\n"
     ]
    }
   ],
   "source": [
    "# Edit below to run policy and value iteration on different configurations\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "    SEED = 1234\n",
    "\n",
    "    RIVER_CURRENT = 'WEAK' # 'WEAK' # 'MEDIUM'\n",
    "    assert RIVER_CURRENT in ['WEAK', 'MEDIUM', 'STRONG']\n",
    "    env = RiverSwim(RIVER_CURRENT, SEED)\n",
    "\n",
    "    R, T = env.get_model()\n",
    "    discount_factor = 0.5\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Policy Iteration\\n\" + \"-\" * 25)\n",
    "\n",
    "    V_pi, policy_pi = policy_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
    "    print(V_pi)\n",
    "    print([['L', 'R'][a] for a in policy_pi])\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Value Iteration\\n\" + \"-\" * 25)\n",
    "\n",
    "    V_vi, policy_vi = value_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
    "    print(V_vi)\n",
    "    print([['L', 'R'][a] for a in policy_vi])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "b6eacba5-f808-4ea7-a8e4-241300b383d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Searching for the largest discount factor such that an optimal agent starting in the initial far-left state does not swim up the river\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "River Current: WEAK\n",
      "Largest Discount Factor:  0.67\n",
      "State Value: [0.015 0.033 0.092 0.257 0.717 1.993]\n",
      "Policy: ['L', 'R', 'R', 'R', 'R', 'R']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    SEED = 1234\n",
    "\n",
    "    RIVER_CURRENT = 'WEAK' # 'WEAK' # 'MEDIUM'\n",
    "    assert RIVER_CURRENT in ['WEAK', 'MEDIUM', 'STRONG']\n",
    "    env = RiverSwim(RIVER_CURRENT, SEED)\n",
    "\n",
    "    R, T = env.get_model()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 135 + \"\\nSearching for the largest discount factor such that an optimal agent starting in the initial far-left state does not swim up the river\\n\" + \"-\" * 135)\n",
    "    \n",
    "    discount_factor = 1\n",
    "    while True:\n",
    "        discount_factor -= 0.01\n",
    "        V_pi, policy_pi = policy_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
    "        if np.all(policy_pi[0] == 0) | (discount_factor < 0):\n",
    "            break\n",
    "            \n",
    "    print(f\"River Current: {RIVER_CURRENT}\")        \n",
    "    print(f\"Largest Discount Factor: {discount_factor: .2f}\")\n",
    "    print(f\"State Value: {V_pi}\")\n",
    "    print(f\"Policy: {[['L', 'R'][a] for a in policy_pi]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "7e472507-7ffc-417e-96dd-4fcd36f8dbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Searching for the largest discount factor such that an optimal agent starting in the initial far-left state does not swim up the river\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "River Current: MEDIUM\n",
      "Largest Discount Factor:  0.77\n",
      "State Value: [0.022 0.035 0.095 0.275 0.798 2.314]\n",
      "Policy: ['L', 'R', 'R', 'R', 'R', 'R']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    SEED = 1234\n",
    "\n",
    "    RIVER_CURRENT = 'MEDIUM' # 'WEAK' # 'MEDIUM'\n",
    "    assert RIVER_CURRENT in ['WEAK', 'MEDIUM', 'STRONG']\n",
    "    env = RiverSwim(RIVER_CURRENT, SEED)\n",
    "\n",
    "    R, T = env.get_model()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 135 + \"\\nSearching for the largest discount factor such that an optimal agent starting in the initial far-left state does not swim up the river\\n\" + \"-\" * 135)\n",
    "    \n",
    "    discount_factor = 1\n",
    "    while True:\n",
    "        discount_factor -= 0.01\n",
    "        V_pi, policy_pi = policy_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
    "        if np.all(policy_pi[0] == 0) | (discount_factor < 0):\n",
    "            break\n",
    "            \n",
    "    print(f\"River Current: {RIVER_CURRENT}\")        \n",
    "    print(f\"Largest Discount Factor: {discount_factor: .2f}\")\n",
    "    print(f\"State Value: {V_pi}\")\n",
    "    print(f\"Policy: {[['L', 'R'][a] for a in policy_pi]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "879d4456-9c2b-4682-bc33-b3c56a45b9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "Searching for the largest discount factor such that an optimal agent starting in the initial far-left state does not swim up the river\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "River Current: STRONG\n",
      "Largest Discount Factor:  0.93\n",
      "State Value: [0.068 0.078 0.146 0.377 1.079 3.169]\n",
      "Policy: ['L', 'R', 'R', 'R', 'R', 'R']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    SEED = 1234\n",
    "\n",
    "    RIVER_CURRENT = 'STRONG' # 'WEAK' # 'MEDIUM'\n",
    "    assert RIVER_CURRENT in ['WEAK', 'MEDIUM', 'STRONG']\n",
    "    env = RiverSwim(RIVER_CURRENT, SEED)\n",
    "\n",
    "    R, T = env.get_model()\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 135 + \"\\nSearching for the largest discount factor such that an optimal agent starting in the initial far-left state does not swim up the river\\n\" + \"-\" * 135)\n",
    "    \n",
    "    discount_factor = 1\n",
    "    while True:\n",
    "        discount_factor -= 0.01\n",
    "        V_pi, policy_pi = policy_iteration(R, T, gamma=discount_factor, tol=1e-3)\n",
    "        if np.all(policy_pi[0] == 0) | (discount_factor < 0):\n",
    "            break\n",
    "            \n",
    "    print(f\"River Current: {RIVER_CURRENT}\")        \n",
    "    print(f\"Largest Discount Factor: {discount_factor: .2f}\")\n",
    "    print(f\"State Value: {V_pi}\")\n",
    "    print(f\"Policy: {[['L', 'R'][a] for a in policy_pi]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebcd35e-5acd-4c9d-889e-3e85c83fb352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
